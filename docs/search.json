[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Old Notes, Musings, and Miscellany",
    "section": "",
    "text": "Foreword\nAfter 22 years of teaching, I have taught many courses and created many iterations of teaching materials. Some of this was “good” and some wasn’t. As the progeny of packrats, I saved a lot of these materials, even the garbage. Now, I have finally begun the process of culling this mess and this work-in-progress is the result.\nMy goal here is to transfer the stuff I feel is reasonable to an online text of sorts. But unlike most texts, this one will likely not be linear, nor perhaps coherent—at least across chapters. If it is useful to you; use whatever you want. If not, ignore it.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#colophon",
    "href": "index.html#colophon",
    "title": "Old Notes, Musings, and Miscellany",
    "section": "Colophon",
    "text": "Colophon\nArtwork by @allison_horst",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Old Notes, Musings, and Miscellany",
    "section": "License",
    "text": "License\nThis work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.",
    "crumbs": [
      "Foreword"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Add introduction at some point.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "02-creating-tables.html",
    "href": "02-creating-tables.html",
    "title": "Creating Tables to Present Statistical Results",
    "section": "",
    "text": "Presenting Summary Statistics\nBelow are some sample tables for presenting statistical information. Although some of them are conventional in the social sciences (e.g., the correlation tables), it is important to remember that these are only examples. You will encounter many variations of these tables as you read scientific work in your substantive area. There may be conventions that are adopted in some areas and not in others. Pay attention to how scholars in your discipline present tabular information.\nThese examples were created using the data sets usnews.csv and riverview.csv.\nStudies often present summary statistics such as the mean and standard deviation. If you only have one or two variables for which you are presenting summary statistics it is often better to present these in the prose of your manuscript. If you have several variables, the information is typically better conveyed in a table. Table 1 is an example table presenting summary statistics for a sample of graduate schools of education.\nTable 1: Means and standard deviations for five measures of graduate programs of education.\n\n\n\n\n \n  \n    Measure \n    M \n    SD \n  \n \n\n  \n    Peer rating \n    3.3 \n    0.5 \n  \n  \n    Acceptance rate for Ph.D. students \n    40.1 \n    20.2 \n  \n  \n    Enrollment \n    969.8 \n    664.9 \n  \n  \n    GRE score (verbal) \n    154.9 \n    3.7 \n  \n  \n    GRE score (quantitative) \n    151.0 \n    4.4\nHere are a few things to note about the table:\nTable 2 shows another table presenting the means and standard deviations, but this time the statistical summaries are conditioned on sex.\nTable 2: Means and standard deviations for three measures of Riverview employees conditioned on sex.\n\n\n\n\n \n\n\nFemales\nNon-females\n\n  \n    Measure \n    M \n    SD \n    M \n    SD \n  \n \n\n  \n    Education level (in years) \n    16.0 \n    4.0 \n    16.0 \n    5.0 \n  \n  \n    Seniority (in years) \n    14.0 \n    7.0 \n    16.0 \n    7.0 \n  \n  \n    Income (in thousand of U.S. dollars) \n    48.9 \n    13.3 \n    59.9 \n    14.2\nTable 2 also presents the means and standard deviations conditioned on sex, but it also includes confidence intervals for the means. It also presents the conditioning in separate rows rather than separate columns. Another possibility, see Table 3, shows the CIs in addition to the means and standard deviations. In this table the conditioning is presented in separate rows rather than separate columns.\nTable 3: Means and confidence intervals (CIs) for three measures of Riverview employees conditioned on sex.\n\n\n\n\n \n\n\n95% CI\n\n  \n    Measure \n    M \n    SD \n    LL \n    UL \n  \n \n\n  \n    Education level (in years) \n     \n     \n     \n     \n  \n  \n         Female \n    16.0 \n    4.0 \n    13.7 \n    17.8 \n  \n  \n         Non-female \n    16.0 \n    5.0 \n    13.5 \n    19.1 \n  \n  \n    Seniority (in years) \n     \n     \n     \n     \n  \n  \n         Female \n    14.0 \n    7.0 \n    10.7 \n    17.5 \n  \n  \n         Non-female \n    16.0 \n    7.0 \n    11.5 \n    19.9 \n  \n  \n    Income (in thousand of U.S. dollars) \n     \n     \n     \n     \n  \n  \n         Female \n    48.9 \n    13.2 \n    42.3 \n    55.5 \n  \n  \n         Non-female \n    59.9 \n    14.2 \n    51.7 \n    68.1",
    "crumbs": [
      "Creating Tables to Present Statistical Results"
    ]
  },
  {
    "objectID": "02-creating-tables.html#presenting-summary-statistics",
    "href": "02-creating-tables.html#presenting-summary-statistics",
    "title": "Creating Tables to Present Statistical Results",
    "section": "",
    "text": "It is numbered/named as “Table X”.\nIt has a caption.\nThe different variables being summarized are presented in the table rows. These are given names that readers can understand. (They are not the variable names used in the dataset which have shortened names like doc_accept and peer.)\nThe different statistics being used to summarize the data are presented in the table columns. These are given abbreviated names as suggested in the APA Publication Manual. In general, statistics are italicized (this is also done in the text of your manuscript).\nThere are no vertical borders in the table. There are horizontal borders above and below the header row, and at the bottom of the table. Other horizontal borders can be included to help readers if the table is particularly long.",
    "crumbs": [
      "Creating Tables to Present Statistical Results"
    ]
  },
  {
    "objectID": "02-creating-tables.html#presenting-correlation-coefficients",
    "href": "02-creating-tables.html#presenting-correlation-coefficients",
    "title": "Creating Tables to Present Statistical Results",
    "section": "Presenting Correlation Coefficients",
    "text": "Presenting Correlation Coefficients\nSimilar to presenting summary statistics, if you only have one or two correlation coefficients to present, it is best to present these in the prose of your manuscript. If you have several correlations the information is typically better conveyed in a table. Table 4 shows an example of a table presenting correlation coefficients for our sample of graduate schools of education.\n\n\n\n\nTable 4: Intercorrelations for five measures of graduate programs of education.\n\n\n\n\n \n  \n    Measure \n    1 \n    2 \n    3 \n    4 \n    5 \n  \n \n\n  \n    1. Peer rating \n    — \n     \n     \n     \n     \n  \n  \n    2. Acceptance rate for Ph.D. students \n    -.54 \n    — \n     \n     \n     \n  \n  \n    3. Enrollment \n    .10 \n    -.03 \n    — \n     \n     \n  \n  \n    4. GRE score (verbal) \n    .43 \n    -.38 \n    .04 \n    — \n     \n  \n  \n    5. GRE score (quantitative) \n    .49 \n    -.39 \n    .08 \n    .81 \n    — \n  \n\n\n\n\n\n\n\n\n\nHere are a few things to note about the table:\n\nThe correlation coefficients are generally rounded to two decimal places.\nIn each column where numbers are presented, the decimal point should be vertically aligned.\nIf the correlation table is to support a regression analysis, typically the outcome variable is the first variable presented in the table (in this case, peer rating). If there is a focal predictor (i.e., a predictor germane to your research question), this should be the second variable presented in the table, etc. Otherwise present the predictors alphabetically.\nDo not indicate “statistical significance” with stars as per the recommendation of the American Statistical Association. Similarly, do not include p-values in a table of correlations. There are many issues related to statistical significance and p-values that arise in a table of correlations, not the least of which is that of multiple tests. It is better to save any presentation of p-values (if you really need to give them) for tables of the regression results.\n\n\nTable 5 is an alternative table presenting both the summary statistics of each variable and the intercorrelations. Combining the information into a single table can be useful when trying to save space in a manuscript.\n\n\n\n\n\nTable 5: Means, standard deivations, and intercorrelations for five measures of graduate programs of education.\n\n\n\n\n \n  \n    Measure \n    M \n    SD \n    1 \n    2 \n    3 \n    4 \n    5 \n  \n \n\n  \n    1. Peer rating \n    3.3 \n    0.5 \n    — \n     \n     \n     \n     \n  \n  \n    2. Acceptance rate for Ph.D. students \n    40.1 \n    20.2 \n    -.54 \n    — \n     \n     \n     \n  \n  \n    3. Enrollment \n    969.8 \n    664.9 \n    .10 \n    -.03 \n    — \n     \n     \n  \n  \n    4. GRE score (verbal) \n    154.9 \n    3.7 \n    .43 \n    -.38 \n    .04 \n    — \n     \n  \n  \n    5. GRE score (quantitative) \n    151.0 \n    4.4 \n    .49 \n    -.39 \n    .08 \n    .81 \n    —",
    "crumbs": [
      "Creating Tables to Present Statistical Results"
    ]
  },
  {
    "objectID": "02-creating-tables.html#presenting-results-from-a-fitted-regression-model",
    "href": "02-creating-tables.html#presenting-results-from-a-fitted-regression-model",
    "title": "Creating Tables to Present Statistical Results",
    "section": "Presenting Results from a Fitted Regression Model",
    "text": "Presenting Results from a Fitted Regression Model\nTypically the results of the “final” adopted model are presented in a table. However, if there are only one or two predictors in the model, it is best to present these in the prose of your manuscript rather than a table. If you have several predictors the information is often better conveyed in a table. Table 6 is an example table presenting the results from a fitted regression model for our sample of graduate schools of education.\n\n\n\n\nTable 6: Unstandardized coefficients for an OLS regression model fitted to estimate variation in peer ratings.\n\n\n\n\n \n  \n    Predictor \n    B \n    SE \n    t \n    p \n  \n \n\n  \n    Constant \n    -0.01 \n    0.00 \n    -5.22 \n    \n  \n\n  \n    Acceptance rate for Ph.D. students \n    0.00 \n    0.00 \n    0.94 \n    0.347 \n  \n  \n    Enrollment \n    0.00 \n    0.02 \n    0.06 \n    0.950 \n  \n  \n    GRE score (verbal) \n    0.04 \n    0.01 \n    2.58 \n    0.011 \n  \n  \n    GRE score (quantitative) \n    -1.86 \n    1.63 \n    -1.14 \n    0.257 \n  \n\n\n\n\n\n\n\n\n\nHere are a few things to note about the table:\n\nThe intercept term of the model (Constant), is relegated to the bottom of the table as it is not typically substantively interesting.\nThe different coefficients are presented in the table rows (when the table includes only one model). These are given names that readers can understand. (They are not the variable names used in the dataset which have shortened names like doc_accept and peer.)\nThere are no stars indicating statistical significance as per the American Statistical Association’s recommendation.\nThe sample size (\\(N\\)) and variance accounted for (\\(R^2\\)) estimates could be added to a footnote or provided in prose.\n\n\nAlternatively, a regression table can include the confidence interval for each of the coefficients in addition to (or in place of!) the p-values. This addresses the uncertainty in the estimates. Table 7 shows such a table.\n\n\n\n\nTable 7: Unstandardized coefficients and confidence intervals for an OLS regression model fitted to estimate variation in peer ratings.\n\n\n\n\n \n\n\n95% CI\n\n  \n    Predictor \n    B \n    SE \n    LL \n    UL \n  \n \n\n  \n    Acceptance rate for Ph.D. students \n    -0.01 \n    0.00 \n    -0.01 \n    -0.01 \n  \n  \n    Enrollment \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    GRE score (verbal) \n    0.00 \n    0.02 \n    -0.03 \n    0.03 \n  \n  \n    GRE score (quantitative) \n    0.04 \n    0.01 \n    0.01 \n    0.06 \n  \n  \n    Constant \n    -1.86 \n    1.63 \n    -5.09 \n    1.37 \n  \n\n\n\n\n\n\n\n\n\nAnother variation on this table includes the standardized regression coefficients (see Table 8).\n\n\n\n\n\nTable 8: Unstandardized and standardized coefficients afor an OLS regression model fitted to estimate variation in peer ratings.\n\n\n\n\n \n  \n    Predictor \n    B \n    β \n    t \n    p \n  \n \n\n  \n    Acceptance rate for Ph.D. students \n    -0.01 \n    -0.41 \n    -5.22 \n    \n  \n\n  \n    Enrollment \n    0.00 \n    0.69 \n    0.94 \n    0.347 \n  \n  \n    GRE score (verbal) \n    0.00 \n    0.007 \n    0.06 \n    0.950 \n  \n  \n    GRE score (quantitative) \n    0.04 \n    0.32 \n    2.58 \n    0.011 \n  \n  \n    Constant \n    -1.86 \n    — \n    -1.14 \n    0.257",
    "crumbs": [
      "Creating Tables to Present Statistical Results"
    ]
  },
  {
    "objectID": "02-creating-tables.html#presenting-results-from-many-fitted-regression-models",
    "href": "02-creating-tables.html#presenting-results-from-many-fitted-regression-models",
    "title": "Creating Tables to Present Statistical Results",
    "section": "Presenting Results from Many Fitted Regression Models",
    "text": "Presenting Results from Many Fitted Regression Models\nIn many analyses, you may need to present the results from a set of fitted models. Here is an example of a table presenting the results from a set of fitted regression models for our sample of graduate schools of education.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 9: Unstandardized coefficients and confidence intervals for a series of OLS regression models fitted to estimate variation in peer ratings.\n\n\n\n\n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\n\n\n\n\nGRE score (verbal)\n\n\n0.011\n\n\n\n\n0.0001\n\n\n\n\n\n\n(-0.024, 0.046)\n\n\n\n\n(-0.031, 0.033)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGRE score (quantitative)\n\n\n0.047\n\n\n\n\n0.036\n\n\n\n\n\n\n(0.017, 0.076)\n\n\n\n\n(0.009, 0.063)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcceptance rate for Ph.D. students\n\n\n\n\n-0.013\n\n\n-0.010\n\n\n\n\n\n\n\n\n(-0.017, -0.009)\n\n\n(-0.014, -0.006)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnrollment\n\n\n\n\n0.0001\n\n\n0.0001\n\n\n\n\n\n\n\n\n(−0.00004, 0.0002)\n\n\n(−0.0001, 0.0002)\n\n\n\n\nConstant\n\n\n-5.488\n\n\n3.769\n\n\n-1.857\n\n\n\n\n\n\n(-8.683, -2.294)\n\n\n(3.572, 3.967)\n\n\n(-5.054, 1.340)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR2\n\n\n0.243\n\n\n0.300\n\n\n0.390\n\n\n\n\nRMSE\n\n\n0.429\n\n\n0.413\n\n\n0.389\n\n\n\n\n\nHere are a few things to note about the table:\n\nWhen presenting the results from multiple models, the goal is to often compare how coefficients differ from model-to-model. Because of this we typically present each model in a column and each coefficient in a row&emdash;this way you compare by reading horizontally.\nIf a model does not include a particular coefficient, leave that cell blank.\nThe intercept term of the model (Constant), is relegated to the bottom of the table as it is not typically substantively interesting.\nUse predictor names that readers can understand. (They are not the variable names used in the dataset which have shortened names like doc_accept and peer.)\nModel-level estimates are also included in the table, typically below the coefficient-level estimates. Sometimes a horizontal line is added to the table as a separator.\n\nIf you have many models to present, use a landscape orientation on your page.\n\nIf you must present p-values, do not include stars indicating statistical significance (as per the American Statistical Association’s recommendation). Include the p-value directly in the table.\n\n\nTable 10: Unstandardized coefficients (standard errors) and p-values for a series of OLS regression models fitted to estimate variation in peer ratings.\n\n\n\n\n\n\nModel A\n\n\nModel B\n\n\nModel C\n\n\n\n\n\n\nGRE score (verbal)\n\n\n0.01(0.02)p=0.531\n\n\n\n\n0.001(0.016)p=0.950\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGRE score (quantitative)\n\n\n0.05(0.02)p=0.003\n\n\n\n\n0.04(0.01)p=0.012\n\n\n\n\n\n\n\n\n\n\n\n\n\nAcceptance rate for Ph.D. students\n\n\n\n\n-0.013(0.002)p&lt;0.001\n\n\n-0.010(0.002)p&lt;0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEnrollment\n\n\n\n\n0.0001(0.0001)p=0.239\n\n\n0.0001(0.0001)p=0.347\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n-5.49(1.63)p=0.002\n\n\n3.77(0.10)p&lt;0.001\n\n\n-1.86(1.63)p=0.258\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR2\n\n\n0.243\n\n\n0.300\n\n\n0.390\n\n\n\n\nRMSE\n\n\n0.429\n\n\n0.413\n\n\n0.389",
    "crumbs": [
      "Creating Tables to Present Statistical Results"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html",
    "href": "02-01-latex-tables.html",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "",
    "text": "YAML\nThis document includes the LaTeX syntax to create the tables from the Creating Tables to Present Statistical Results document. You can download the original QMD file here or view a rendered PDF here.\nAt the beginning of the QMD document, the YAML I used is below. The header-includes: line loads the {xfrac} LaTeX package. The \\floatplacement{table}[H] is used to place the table in the rendered document exactly where the code appears rather than floating the table.",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#yaml",
    "href": "02-01-latex-tables.html#yaml",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "",
    "text": "---\ntitle: \"Creating Tables - LaTeX Tables\"\nformat: \n  pdf:\n    highlight: tango\n    latex_engine: xelatex\n    fig_width: 6\n    fig_height: 6\nheader-includes:\n   - \\usepackage{xfrac}\n   - \\floatplacement{table}{H}\nmainfont: \"Sabon\"\nsansfont: \"Helvetica Neue UltraLight\"\nmonofont: Inconsolata\nalways_allow_html: yes\n---",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#setup-chunk",
    "href": "02-01-latex-tables.html#setup-chunk",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Setup Chunk",
    "text": "Setup Chunk\nThe setup chunk loads the R packages, sets different knitr and R options, and imports the data used in the tables.\n\n```{r}\n#| label: setup\n#| message: false\n#| warning: false\n\n# Load libraries\nlibrary(corrr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\nlibrary(tidyr)\n\n# Ignore NAs and replace with blank space\noptions(knitr.kable.NA = '')\n\n# Import data\nusnews = read_csv(file = \"https://raw.githubusercontent.com/zief0002/musings/refs/heads/main/data/ed-schools-2018.csv\") |&gt; drop_na()\nriverview = read_csv(file = \"https://raw.githubusercontent.com/zief0002/musings/refs/heads/main/data/riverview.csv\")\n```",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#table-1",
    "href": "02-01-latex-tables.html#table-1",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Table 1",
    "text": "Table 1\nThe key to these tables is to understand that the information in the tables’ cells needs to be included in a data frame or tibble. This information can either be typed in manually or computation on the data can produce a data frame. For example, the data in Table 1 was entered manually:\n\n# Set up data frame\ndata.frame(\n  Measure = c(\"Peer rating\", \"Acceptance rate for Ph.D. students\", \"Enrollment\", \"GRE score (verbal)\", \"GRE score (quantitative)\"),\n  M  = c(mean(usnews$peer), mean(usnews$doc_accept), mean(usnews$enroll), \n         mean(usnews$gre_verbal), mean(usnews$gre_quant)),\n  SD = c(sd(usnews$peer), sd(usnews$doc_accept), sd(usnews$enroll), \n         sd(usnews$gre_verbal), sd(usnews$gre_quant))\n)\n\n                             Measure          M          SD\n1                        Peer rating   3.312295   0.4893203\n2 Acceptance rate for Ph.D. students  40.113115  20.2276300\n3                         Enrollment 969.762295 664.9454219\n4                 GRE score (verbal) 154.860656   3.7090810\n5           GRE score (quantitative) 150.991803   4.4154049\n\n\nThe resulting data frame includes all of the data in the table…just without table formatting, etc. This data frame is then piped into the kable() function (which is in the {knitr} package). This function does all of the formatting, etc. by setting different arguments.\n\nkable(\n    col.names = c(\"Measure\", \"\\\\textit{M}\", \"\\\\textit{SD}\"),\n    align = c(\"l\", \"c\", \"c\"),\n    digits = 1,\n    format = \"latex\",\n    booktabs = TRUE,\n    escape = FALSE\n    )\n\n\nThe argument col.names= indicates the column names to be used the table’s header row. The \\\\textit{} is an escaped LaTeX command to make everything in the curly braces italics. The actual command is \\textit{} but the initial \\ is an escape character to ensure the LaTeX command is interpreted properly.\nThe align= argument sets the alignment within each of the columns. In this case, the content in the first column is left-aligned and the content in the remaining two columns are centered.\ndigits= sets the rounding. Here 1 sets the rounding to a single decimal place for all numerical columns. You could also use a c() function to specify different rounding for each column.\nThe format=\"latex\" indicates the resulting output should be LaTeX syntax.\nThe booktabs=TRUE argument uses the {booktabs} LaTeX package for better formatting.\nLastly, the escape=FALSE argument allows us to use LaTeX commands in the code (e.g., in the col.names= argument)…but, they need to be properly escaped with the \\ character.\n\nThe captioning of the table is done with the code chunk option tbl-cap:.\n```{r}\n#| label: tbl-01\n#| tbl-cap: \"Means and standard deviations for five measures of graduate programs of education.\"\n\n\n# Set up data frame\ndata.frame(\n  Measure = c(\"Peer rating\", \"Acceptance rate for Ph.D. students\", \"Enrollment\", \"GRE score (verbal)\", \"GRE score (quantitative)\"),\n  M  = c(mean(usnews$peer), mean(usnews$doc_accept), mean(usnews$enroll), \n         mean(usnews$gre_verbal), mean(usnews$gre_quant)),\n  SD = c(sd(usnews$peer), sd(usnews$doc_accept), sd(usnews$enroll), \n         sd(usnews$gre_verbal), sd(usnews$gre_quant))\n) |&gt;\n  kable(\n    col.names = c(\"Measure\", \"\\\\textit{M}\", \"\\\\textit{SD}\"),\n    align = c(\"l\", \"c\", \"c\"),\n    digits = 1,\n    format = \"latex\",\n    booktabs = TRUE,\n    escape = FALSE\n    )\n```\nFinally, note that is you ran the syntax for Table 1 in the console, the output is LaTeX syntax that creates the table when rendered via a LaTeX engine. The actual LaTeX syntax to create Table 1 is below.\n```{r}\n\\begin{tabular}{lcc}\n\\toprule\nMeasure & \\textit{M} & \\textit{SD}\\\\\n\\midrule\nPeer rating & 3.3 & 0.5\\\\\nAcceptance rate for Ph.D. students & 40.1 & 20.2\\\\\nEnrollment & 969.8 & 664.9\\\\\nGRE score (verbal) & 154.9 & 3.7\\\\\nGRE score (quantitative) & 151.0 & 4.4\\\\\n\\bottomrule\n\\end{tabular}\n```\nLooking at the resulting table and the syntax, you might be able to discern what some of this syntax does:\n\n& separates the content in a row into different columns\n\\\\ is a newline indicator\n\\toprule, \\midrule, and \\bottomrule add horizontal rules (lines) in the table",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#table-2",
    "href": "02-01-latex-tables.html#table-2",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Table 2",
    "text": "Table 2\nTable 2’s syntax is quite similar to that for Table 1. The only additional syntax is the add_header_above() function (from the {kableExtra} package). This adds a second header row above the original header row. In this case this second header includes blank space above column 1, the label “Females” above column 2, and the label “Males” above column 3.\n```{r}\n#| label: tbl-02\n#| tbl-cap: \"Means and standard deviations for three measures of Riverview employees conditioned on sex.\"\n#| echo: false\n\n# Set up data frame\ndata.frame(\n  Measure = c(\"Education level (in years)\", \"Seniority (in years)\", \"Income (in U.S. dollars\"),\n  M_f  = c(\n    mean(riverview$education[riverview$gender == \"female\"]),\n    mean(riverview$seniority[riverview$gender == \"female\"]),\n    mean(riverview$income[riverview$gender == \"female\"])\n  ),\n  SD_f = c(\n    sd(riverview$education[riverview$gender == \"female\"]),\n    sd(riverview$seniority[riverview$gender == \"female\"]),\n    sd(riverview$income[riverview$gender == \"female\"])\n  ),\n  M_m  = c(\n    mean(riverview$education[riverview$gender == \"male\"]),\n    mean(riverview$seniority[riverview$gender == \"male\"]),\n    mean(riverview$income[riverview$gender == \"male\"])\n  ),\n  SD_m = c(\n    sd(riverview$education[riverview$gender == \"male\"]),\n    sd(riverview$seniority[riverview$gender == \"male\"]),\n    sd(riverview$income[riverview$gender == \"male\"])\n  )\n) |&gt;\n  kable(\n    col.names = c(\"Measure\", \"\\\\textit{M}\", \"\\\\textit{SD}\", \"\\\\textit{M}\", \"\\\\textit{SD}\"),\n    align = c(\"l\", \"c\", \"c\", \"c\", \"c\"),\n    digits = 0,\n    format = \"latex\",\n    booktabs = TRUE,\n    escape = FALSE\n  ) |&gt;\n  add_header_above(\n    header = c(\" \" = 1, \"Females\" = 2, \"Males\" = 2)\n  )\n```",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#table-4",
    "href": "02-01-latex-tables.html#table-4",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Table 4",
    "text": "Table 4\nTo create the data frame for Table 4, rather thn manually entering the information, we are using the correlate() function to produce the content in the table. The output from this looks like it is formatted correctly, but is not really a dat frame. So to fix that we pipe that output into the data.frame() function.\n```{r}\n#| label: tbl-04\n#| tbl-cap: \"Intercorrelations for five measures of graduate programs of education.\"\n#| echo: false\n#| message: false\n\ntab_04 = usnews |&gt;\n  select(peer, doc_accept, enroll, gre_verbal, gre_quant) |&gt;\n  correlate() |&gt;\n  shave(upper = TRUE) |&gt;\n  fashion(decimals = 2, na_print = \"\")  |&gt;\n  data.frame()\n\ntab_04[1, 2] = \"---\"\ntab_04[2, 3] = \"---\"\ntab_04[3, 4] = \"---\"\ntab_04[4, 5] = \"---\"\ntab_04[5, 6] = \"---\"\n\ntab_04 |&gt;\n  mutate(\n    term = c(\"1. Peer rating\", \"2. Acceptance rate for Ph.D. students\", \"3. Enrollment\",\n                \"4. GRE score (verbal)\", \"5. GRE score (quantitative)\")\n  ) |&gt;\n  kable(\n    col.names = c(\"Measure\", \"1\", \"2\", \"3\", \"4\", \"5\"),\n    align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n    format = \"latex\",\n    booktabs = TRUE,\n    escape = FALSE\n  )\n```",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#table-5",
    "href": "02-01-latex-tables.html#table-5",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Table 5",
    "text": "Table 5\nThe syntax here is similar to Table 4. We also set up a data fame manually that includes the means and SDs. This data frame is then combined with the correlation data frame using cbind()—which is short for column bind.\n```{r}\n#| label: tbl-05\n#| tbl-cap: \"Means, standard deivations, and intercorrelations for five measures of graduate programs of education.\"\n#| echo: false\n#| message: false\n\ntab_05_2 = usnews |&gt;\n  select(peer, doc_accept, enroll, gre_verbal, gre_quant) |&gt;\n  correlate() |&gt;\n  shave(upper = TRUE) |&gt;\n  fashion(decimals = 2, na_print = \"—\")\n\ntab_05_1 = data.frame(\n  M = c(3.3, 40.1, 970, 154.9, 151.0),\n  SD = c(0.5, 20.2, 665, 3.7, 4.4)\n)\n\ntab_05 = cbind(tab_05_2[, 1], tab_05_1, tab_05_2[, 2:6])\n\ntab_05[1, 5:8] = \"\"\ntab_05[2, 6:8] = \"\"\ntab_05[3, 7:8] = \"\"\ntab_05[4, 8] = \"\"\n\ntab_05 |&gt;\n  mutate(\n    `tab_05_2[, 1]` = c(\"1. Peer rating\", \"2. Acceptance rate for Ph.D. students\", \"3. Enrollment\", \"4. GRE score (verbal)\", \"5. GRE score (quantitative)\")\n  ) |&gt;\n  kable(\n    col.names = c(\"Measure\", \"\\\\textit{M}\", \"\\\\textit{SD}\", \"1\", \"2\", \"3\", \"4\", \"5\"),\n    align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n    format = \"latex\",\n    booktabs = TRUE,\n    escape = FALSE\n  )\n```",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#table-6",
    "href": "02-01-latex-tables.html#table-6",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Table 6",
    "text": "Table 6\nHere, again, we use R functions to produce the table content—namely the tidy() function from the {broom} package. The output from tidy() is formatted as a data frame, so there is no need to pipe it into data.frame(); it can be piped directly into kable().\n#| label: tbl-06\n#| tbl-cap: \"Unstandardized coefficients for an OLS regression model fitted to estimate variation in peer ratings.\"\n#| echo: false\n\ntab_06 = lm(peer ~ 1 + doc_accept + enroll + gre_verbal + gre_quant, data = usnews) |&gt;\n  broom::tidy() |&gt;\n  mutate(\n    term = c(\"Constant\", \"Acceptance rate for Ph.D. students\", \"Enrollment\",\n             \"GRE score (verbal)\", \"GRE score (quantitative)\")\n  )\n\ntab_06 = tab_06[c(2:5, 1), ]\n\ntab_06 |&gt;\n  kable(\n    col.names = c(\"Predictor\", \"\\\\textit{B}\", \"\\\\textit{SE}\", \"\\\\textit{t}\", \"\\\\textit{p}\"),\n    align = c(\"l\", \"c\", \"c\", \"c\", \"c\"),\n    digits = c(NA, 2, 2, 2, 3),\n    format = \"latex\",\n    booktabs = TRUE,\n    escape = FALSE\n  )",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#table-7",
    "href": "02-01-latex-tables.html#table-7",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Table 7",
    "text": "Table 7\nNothing new here. Just a variation on Table 6.\n```{r}\n#| label: tbl-07\n#| tbl-cap: \"Unstandardized coefficients and confidence intervals for an OLS regression model fitted to estimate variation in peer ratings.\"\n#| echo: false\n\ntab_07 = lm(peer ~ 1 + doc_accept + enroll + gre_verbal + gre_quant, data = usnews) |&gt;\n  broom::tidy(conf.int = TRUE) |&gt;\n  mutate(\n    term = c(\"Constant\", \"Acceptance rate for Ph.D. students\", \"Enrollment\",\n             \"GRE score (verbal)\", \"GRE score (quantitative)\")\n  )\n\ntab_07[c(2:5, 1), ] |&gt;\n  select(term, estimate, std.error, conf.low, conf.high)  |&gt;\n  kable(\n    col.names = c(\"Predictor\", \"\\\\textit{B}\", \"\\\\textit{SE}\", \"\\\\textit{LL}\", \"\\\\textit{UL}\"),\n    align = c(\"l\", \"c\", \"c\", \"c\", \"c\"),\n    digits = c(NA, 2, 2, 2, 2),\n    format = \"latex\",\n    booktabs = TRUE,\n    escape = FALSE\n  ) |&gt;\n  row_spec(row = 0, align = \"c\") |&gt;\n  add_header_above(c(\" \" = 3, \"95% CI\" = 2))\n```",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#table-8",
    "href": "02-01-latex-tables.html#table-8",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Table 8",
    "text": "Table 8\nAgain, nothing new here. The \\\\beta create the Greek lower-case letter beta.\n```{r}\n#| label: tbl-08\n#| tbl-cap: \"Unstandardized and standardized coefficients afor an OLS regression model fitted to estimate variation in peer ratings.\"\n#| echo: false\n\ntab_08 = broom::tidy(lm(peer ~ 1 + doc_accept + enroll + gre_verbal + gre_quant, data = usnews))\n\ntab_08[c(2:5, 1), ] |&gt;\n  mutate(\n    term = c(\"Acceptance rate for Ph.D. students\", \"Enrollment\", \"GRE score (verbal)\",\n             \"GRE score (quantitative)\", \"Constant\"),\n    beta = c(\"-0.41\", \"0.69\", \"0.007\", \"0.32\", \"---\"),\n    p.value = scales::pvalue(p.value, accuracy = .001)\n  ) |&gt;\n  select(term, estimate, beta, statistic, p.value) |&gt;\n  kable(\n    col.names = c(\"Predictor\", \"\\\\textit{B}\", \"$\\\\beta$\", \"\\\\textit{t}\", \"\\\\textit{p}\"),\n    align = c(\"l\", \"c\", \"c\", \"c\", \"c\"),\n    digits = c(NA, 2, 2, 2, 3),\n    format = \"latex\",\n    booktabs = TRUE,\n    escape = FALSE\n  )\n```",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#table-9",
    "href": "02-01-latex-tables.html#table-9",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Table 9",
    "text": "Table 9\nFor Table 9, we use the stargazer() function from the {stagazer} package to initially create our table content. However, I don’t love the exact output of the table. So rather than just using the rendered output, I run the syntax in the console to get the LaTeX syntax, and then copy-and-paste that syntax in my Quarto document (not in a code chunk) and modify it manually. I add the eval: false code chunk so that I can keep the syntax for reprodcibility, but when I render the document the table won’t be produced from the stargazer() function.\n```{r}\n#| echo: false\n#| message: false\n#| results: \"asis\"\n#| eval: false\n\nlibrary(stargazer)\n\nlm.1 = lm(peer ~ 1 + gre_verbal + gre_quant, data = usnews)\nlm.2 = lm(peer ~ 1 + doc_accept + enroll, data = usnews)\nlm.3 = lm(peer ~ 1 + doc_accept + enroll + gre_verbal + gre_quant, data = usnews)\n\nstargazer(lm.1, lm.2, lm.3,\n          ci = TRUE,\n          column.labels = c(\"Model 1\", \"Model 2\", \"Model 3\"),\n          covariate.labels = c(\"GRE score (verbal)\", \"GRE score (quantitative)\", \"Acceptance rate for Ph.D. students\", \"Enrollment\"),\n          dep.var.caption = \"Outcome variable: Peer ratings\",\n          dep.var.labels = NULL,\n          dep.var.labels.include = FALSE,\n          type = \"latex\",\n          keep.stat = c(\"rsq\",\"ser\"),\n          star.cutoffs = NA,\n          header = FALSE,\n          table.placement = 'H',\n          title = \"Unstandardized Coefficients and Confidence Intervals for a Series of Regression Models Fitted to Data from $n=129$ Graduate Schools of Education to Predict Variation in Peer Ratings\",\n          omit.table.layout = \"n\"\n)\n```\nThe modified syntax I have in the document is as follows:\n```{r}\n\\begin{table}[H]\n\\caption{Unstandardized coefficients and confidence intervals for a series of OLS regression models fitted to estimate variation in peer ratings.}\n\\label{}\n\\begin{tabular}{@{\\extracolsep{5pt}}lccc}\n\\\\[-1.8ex]\\hline\\\\[1ex]\n& Model 1 & Model 2 & Model 3 \\\\\n\\hline \\\\[-1.8ex]\nGRE score (verbal) & 0.011 &  & 0.001 \\\\\n& ($-$0.024, 0.046) &  & ($-$0.031, 0.033) \\\\\n& & & \\\\\nGRE score (quantitative) & 0.047 &  & 0.036 \\\\\n& (0.017, 0.076) &  & (0.009, 0.063) \\\\\n& & & \\\\\nAcceptance rate for Ph.D. students &  & $-$0.013 & $-$0.010 \\\\\n&  & ($-$0.017, $-$0.009) & ($-$0.014, $-$0.006) \\\\\n& & & \\\\\nEnrollment &  & 0.0001 & 0.0001 \\\\\n&  & ($-$0.00004, 0.0002) & ($-$0.0001, 0.0002) \\\\\n& & & \\\\\nConstant & $-$5.488 & 3.769 & $-$1.857 \\\\\n& ($-$8.683, $-$2.294) & (3.572, 3.967) & ($-$5.054, 1.340) \\\\\n& & & \\\\\n\\hline \\\\[-1.8ex]\nR$^{2}$ & 0.243 & 0.300 & 0.390 \\\\\nRMSE & 0.429 & 0.413 & 0.389 \\\\\n\\hline\n\\end{tabular}\n\\end{table}\n```",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "02-01-latex-tables.html#table-10",
    "href": "02-01-latex-tables.html#table-10",
    "title": "Creating Tables: LaTeX and Quarto",
    "section": "Table 10",
    "text": "Table 10\nI used the same process for Table 10 as I did for Table 9\n```{r}\n#| echo: false\n#| message: false\n#| results: \"asis\"\n#| eval: false\n\nstargazer(lm.1, lm.2, lm.3,\n          column.labels = c(\"Model 1\", \"Model 2\", \"Model 3\"),\n          covariate.labels = c(\"GRE score (verbal)\", \"GRE score (quantitative)\", \"Acceptance rate for Ph.D. students\", \"Enrollment\"),\n          dep.var.caption = \"Outcome variable: Peer ratings\",\n          dep.var.labels = NULL,\n          dep.var.labels.include = FALSE,\n          type = \"latex\",\n          keep.stat = c(\"rsq\",\"ser\"),\n          report = c(\"vcsp\"),\n          star.cutoffs = NA,\n          header = FALSE,\n          table.placement = 'H',\n          title = \"Unstandardized Coefficients and Confidence Intervals for a Series of Regression Models Fitted to Data from $n=129$ Graduate Schools of Education to Predict Variation in Peer Ratings\",\n          omit.table.layout = \"n\"\n)\n```\nAnd the modified syntax for Table 10 is:\n```{r}\n\\begin{table}[H]\n\\caption{Unstandardized coefficients (standard errors) and $p$-Values for a series of OLS regression models fitted to estimate variation in peer ratings.}\n\\label{}\n\\begin{tabular}{@{\\extracolsep{5pt}}lccc}\n\\\\[-1.8ex]\\hline\\\\[1ex]\n& Model 1 & Model 2 & Model 3 \\\\\n\\hline \\\\[-1.8ex]\nGRE score (verbal) & 0.011 &  & 0.001 \\\\\n& (0.018) &  & (0.016) \\\\\n& $p = 0.531$ &  & $p = 0.950$ \\\\\n& & & \\\\\nGRE score (quantitative) & 0.047 &  & 0.036 \\\\\n& (0.015) &  & (0.014) \\\\\n& $p = 0.003$ &  & $p = 0.012$ \\\\\n& & & \\\\\nAcceptance rate for Ph.D. students &  & $-$0.013 & $-$0.010 \\\\\n&  & (0.002) & (0.002) \\\\\n&  & $p = 0.000$ & $p = 0.00000$ \\\\\n& & & \\\\\nEnrollment &  & 0.0001 & 0.0001 \\\\\n&  & (0.0001) & (0.0001) \\\\\n&  & $p = 0.239$ & $p = 0.347$ \\\\\n& & & \\\\\nConstant & $-$5.488 & 3.769 & $-$1.857 \\\\\n& (1.630) & (0.101) & (1.631) \\\\\n& $p = 0.002$ & $p = 0.000$ & $p = 0.258$ \\\\\n& & & \\\\\n\\hline \\\\[-1.8ex]\nR$^{2}$ & 0.243 & 0.300 & 0.390 \\\\\nRMSE & 0.429 & 0.413 & 0.389 \\\\\n\\hline\n\\hline \\\\[-1.8ex]\n\\end{tabular}\n\\end{table}\n```",
    "crumbs": [
      "Creating Tables: LaTeX and Quarto"
    ]
  },
  {
    "objectID": "04-kde.html",
    "href": "04-kde.html",
    "title": "Kernel Density Estimation",
    "section": "",
    "text": "Loading Packages and Importing the Data\nAdapted from Zieffler et al. (2011)\nIn 1986, the Vietnamese government began a policy of doi moi (renovation) and decided to move from a centrally planned command economy to a “market economy with socialist direction.” As a result, Vietnam was able to evolve from near famine conditions in 1986 to a position as the world’s third largest exporter of rice in the mid-1990s. Between 1992 and 1997 Vietnam’s gross domestic product (GDP) rose by 8.9% annually (WorldBank, 1999).\nThe first Vietnam Living Standards Survey (VLSS) was conducted in 1992–93 by the State Planning Committee (SPC) (now Ministry of Planning and Investment) along with the General Statistical Office (GSO). The second VLSS was conducted by the GSO in 1997–98. The survey was part of the Living Standards Measurement Study (LSMS) that was conducted in a number of developing countries with technical assistance from the World Bank.\nThe second VLSS was designed to provide an up-to-date source of data on households to be used in policy design, monitoring of living standards, and evaluation of policies and programs. One part of the evaluation was whether the policies and programs that were currently available were age appropriate for the population. For example, if a country has a higher proportion of older people, then there needs to be programs available that appeal to that sector of the population. Another concern was whether the living standards for different sections of the country were equitable.\nGiven the background above, data from the second VLSS (available in the vlss-age.csv data set) is used to examine the following research question:\nTo address this research question, we will use kernel density estimation to visually explore the age distribution in the sample data.\nTo begin, we will load two packages that we will use in this analysis.\n# Load libraries\nlibrary(quantreg)\nlibrary(tidyverse)\nThe vlss-age.csv data contains the ages of 28,633 individuals (in years ranging from 0 to 99). We will import this data using the read_csv() function from the tidyverse package1.\n# Import data\nvlss = read_csv(\"https://raw.githubusercontent.com/zief0002/musings/master/data/vlss-age.csv\")\n\n# View data\nvlss\n\n# A tibble: 28,633 × 1\n     age\n   &lt;dbl&gt;\n 1    68\n 2    70\n 3    31\n 4    28\n 5    22\n 6     7\n 7    57\n 8    27\n 9    23\n10     0\n# ℹ 28,623 more rows",
    "crumbs": [
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "04-kde.html#nonparametric-density-estimation",
    "href": "04-kde.html#nonparametric-density-estimation",
    "title": "Kernel Density Estimation",
    "section": "Nonparametric Density Estimation",
    "text": "Nonparametric Density Estimation\nDensity estimation is one of the most useful methods for examining sample distributions. Nonparametric density estimation is an approach to estimating the distribution of the population from which the sample was drawn. This allows the observed sample data to provide insight into the structure of the unknown probability distribution underlying the population. The density estimation is nonparametric in the sense that the sample data suggest the shape, rather than imposing the shape of a known population distribution with particular values of its parameters.\n\n\n\n\n\nFigure 1: Histogram of the age distribution.\n\n\n\n\n\n\n\n\nIn the past, it was common for behavioral scientists to use a histogram to estimate the population density. A histogram is one of the simplest nonparametric estimators of a population density. Histograms, usually displayed as a graph, are essentially enumerations, or counts, of the observed data for each of a number of disjoint categories, or bins. Despite the popularity of the histogram, it has a number of drawbacks that suggest it is not always the best method to use.\nThe bin width—which is often chosen by the software rather than the researcher—has a great deal of influence on the shape of the histogram, and thus, on the inferences made by the researcher. Most methods for creating histograms partition the observed data into equally spaced bins using algorithms that typically focus on producing the optimal bin count given the observed data (Freedman & Diaconis, 1981; Scott, 1979; Sturges, 1926). These methods often have strong assumptions about the shape of the underlying population distribution.\nFor example, when distributions are strongly skewed, the density estimate produced by the histogram may be misleading, since several of the bins chosen in the optimization process can have very little data (e.g., Rissanen et al., 1992). While there have been methodological advances in selecting the bin width in a histogram (e.g., Wand, 1997), histograms are still essentially modeling the data using discontinuous step functions. These functions assume gaps in the underlying population density (i.e., a certain discontinuity or discreteness) when the graph of the function jumps to each new x-value. So, if the researcher believes that the observed sample data comes from a population with a continuous density, then other estimation methods are preferable. Fortunately, there are substantially better methods of estimating populations with a continuous density.",
    "crumbs": [
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "04-kde.html#kernel-density-estimators",
    "href": "04-kde.html#kernel-density-estimators",
    "title": "Kernel Density Estimation",
    "section": "Kernel Density Estimators",
    "text": "Kernel Density Estimators\nA much better estimate of the population density for continuous variables can be obtained by using kernel methods. Nonparametric kernel density estimation can be thought of as a method of averaging and smoothing the density estimate provided by the histogram. More formally, kernel density estimation is a sophisticated form of locally weighted averaging of the sample distribution.\nKernel density estimation works by estimating the density at each observation, x, using a smooth, weighted function, known as a kernel. The figure below shows a conceptual illustration of kernel density estimation (adapted from Sain, 1994). The vertical lines below the axis represent the \\(N=6\\) sample observations. The dashed lines represent the Gaussian kernel function, and the solid line represents the overall density estimate. The smoothing parameter, represented by the variation in each distribution, is fixed (i.e., it is the same value in each kernel).\n\n\n\n\n\nFigure 2: Illustration of the kernel density estimation (solid line) for N=6 observations (vertical lines). A Gaussian kernel function (dashed lines) with a fixed smoothing parameter was centered at each observation. The figure was adapted from Sain (1994).\n\n\n\n\n\n\n\n\nIn the figure, the kernel function is the normal, or Gaussian, distribution.2 Each kernel function is centered at one of the \\(N=6\\) observations and identically scaled. An estimate of the overall density can then be found by summing the height of all the kernel densities at each observation.\nNote that the variation (width) in the kernel determines the amount of overlap at each observed value. Skinnier kernels have less overlap resulting in a smaller overall sum, while wider kernels would result in more overlap and a larger sum. The data analyst not only specifies the kernel function, but also the variation in the kernel function. The kernel density illustration shows a visual depiction of the variation in the kernel functions based on the half-width of the kernel (i.e., half the width of each kernel). This is also referred to as the smoothing parameter since it has a direct impact on the overall smoothness of the plot.",
    "crumbs": [
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "04-kde.html#plotting-a-kernel-density-estimate",
    "href": "04-kde.html#plotting-a-kernel-density-estimate",
    "title": "Kernel Density Estimation",
    "section": "Plotting a Kernel Density Estimate",
    "text": "Plotting a Kernel Density Estimate\nTo plot a kernel density estimate of a distribution, we will use the geom_density() function from the {tidyverse} package3.\n\n# Create density plot\nggplot(data = vlss, aes(x = age)) +\n  geom_density() +\n  theme_bw() +\n  xlab(\"Age\") +\n  ylab(\"Density\")\n\n\n\n\nFigure 3: Kernel density plot of the age distribution using the default (Gaussian) kernel.\n\n\n\n\n\n\n\n\nChanging either the kernel function or the smoothing parameter affects the overall density that is estimated. There are several kernel functions that can be used in R. You can change the kernel function by adding the kernel= argument to geom_density() layer. For example, to use the Epanechnikov kernel, we would add kernel=\"epanechnikov\" to the geom_density() layer.\n\n# Change kernel function\nggplot(data = vlss, aes(x = age)) +\n  geom_density(kernel=\"epanechnikov\") +\n  theme_bw() +\n  xlab(\"Age\") +\n  ylab(\"Density\")\n\n\n\n\nFigure 4: Kernel density plot of the age distribution using the Epanechnikov kernel. Changing the kernel has little effect on the estimation of the density; this plot looks almost identical to the density plot using the Gaussian kernel.\n\n\n\n\n\n\n\n\nChanging the kernel function uses a different shaped distribution to estimate the density at each observation. The methodological research literature suggests that the choice of the functional form of the kernel has little effect on the estimation of the density—all are essentially equivalently efficient in minimizing the error when approximating the true density (e.g., Epanechnikov, 1969; Silverman, 1986)—and thus it is reasonable to simply use the default Gaussian kernel function.\nSelecting the smoothing parameter is another matter. Change in the amount of variation in the kernel has a large effect on the appearance and interpretation of the density estimate. This is similar to forcing fewer or additional bins in the traditional histogram. The figure below shows the difference in appearance in the overall plot of the kernel density that is associated with changing the smoothing parameter. The three density estimates are based on the same sample of data, and all use the Gaussian kernel function. It can be seen that using a smoothing parameter of 0.5 produces an estimate of the density which is quite rough (left-hand plot), while using a smoothing parameter of 2 (middle plot) produces a smoother estimate. A smoothing parameter of 10 (right-hand plot) produces an even smoother plot. As the smoothing parameter increases, the density curve becomes smoother.\n\n\n\n\n\nFigure 5: Kernel density estimates using three different smoothing parameters. Each estimate used a Gaussian kernel. The estimate on the left used a smoothing parameter of 0.5. The estimate in the middle used a smoothing parameter of 2. The estimate on the right used a smoothing parameter of 10.\n\n\n\n\n\n\n\n\nTo set the smoothing parameter, we use the bw= argument in the geom_density() layer of the plot. For example, to set the smoothing parameter to 10, we would use the following syntax:\n\n# Change smoothing parameter\nggplot(data = vlss, aes(x = age)) +\n  geom_density(bw = 10) +\n  theme_bw() +\n  xlab(\"Age\") +\n  ylab(\"Density\")\n\nIf the researcher chooses a smoothing parameter that is too small, the density estimate will appear jagged, spuriously highlighting anomalies of the data such as asymmetry and multiple modes. Such features can appear because of chance variation rather than because they are structures present in the probability distribution. If the researcher chooses a smoothing parameter that is too large, she may obscure much of the structure in the data, a phenomenon known as oversmoothing. Ideally a smoothing parameter is chosen that is small enough to reveal detail in the graph but large enough to inhibit random noise.\nSeveral methods have been proposed to choose an optimum smoothing parameter based on the data (see Sheather, 2004). While these methods tend to compute smoothing parameters that perform well in simulation studies, for sample data that is substantially nonnormal, some manual adjustment may be required. The method that the geom_density() function uses to compute the value of the smoothing parameter is based on Silverman’s rule of thumb (Silverman, 1986). The method popularized by Silverman—which was originally proposed for improved density estimation in histograms (e.g., Deheuvels, 1977; Scott, 1979)—is a computationally simple method for choosing the smoothing parameter. The geom_density() function computes the density based on several popular methods:\n\nSilverman’s method (default); bw=\"nrd0\"\nScott’s method; bw=\"nrd\"\nSheather and Jones’ method; bw=\"sj\"\nUnbiased cross-validation; bw=\"ucv\"\nBiased cross-validation; bw=\"bcv\"\n\nThis actual value of the smoothing parameter can be obtained by employing the density() function.\n\n# Obtain smoothing parameter for Silverman's method\nd = density(vlss$age, bw = \"nrd0\")\nd\n\n\nCall:\n    density.default(x = vlss$age, bw = \"nrd0\")\n\nData: vlss$age (28633 obs.);    Bandwidth 'bw' = 2.344\n\n       x                 y            \n Min.   : -7.033   Min.   :9.980e-08  \n 1st Qu.: 21.234   1st Qu.:1.406e-03  \n Median : 49.500   Median :6.410e-03  \n Mean   : 49.500   Mean   :8.827e-03  \n 3rd Qu.: 77.766   3rd Qu.:1.384e-02  \n Max.   :106.033   Max.   :2.585e-02  \n\n\nFor Silverman’s method, the default used in the geom_density() function, the numerical value of the smoothing parameter for the age variable is 2.344. Although the default smoothing parameter in this case seems reasonable in providing a smooth plot, the prudent data analyst should try several smoothing parameters to ensure that the plot is not oversmoothed. The figure below shows the original plot, as well as additional kernel density plots using the Sheather and Jones method of computing the smoothing parameter. All the methods yield similar curves for the age variable.\n\n\n\n\n\nFigure 6: Kernel density estimates using Silverman’s rule of thumb for the smoothing parameter (left-hand plot) and Sheather and Jones’ method of computing the smoothing parameter, (right-hand plot).",
    "crumbs": [
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "04-kde.html#adaptive-kernel-density-estimation",
    "href": "04-kde.html#adaptive-kernel-density-estimation",
    "title": "Kernel Density Estimation",
    "section": "Adaptive Kernel Density Estimation",
    "text": "Adaptive Kernel Density Estimation\nOne promising method for computing the degree of smoothing is the use of adaptive kernel estimation (e.g., Bowman & Azzalini, 1997; Silverman, 1986; Terrell & Scott, 1992). This method seems especially well-suited for estimating densities of long-tailed or multi-modal data. Adaptive kernel density estimation allows the kernels to have differing widths. Varying the smoothing parameter reduces the potential of undersmoothing the density estimate where there are sparse data and also reduces the potential of oversmoothing the density estimate where there are heavy data.\nAdaptive kernel densities can be estimated using the akj() function from the {quantreg} package. The z= argument takes an equispace sequence of values for which the density should be estimated. From the computations earlier, recall that the density object, d, contains a set of equispaced \\(x\\)-values at which the density was estimated. These can be accessed using d$x, which is then supplied to the z= argument in the akj() function. The estimated density is assigned to the object adapt. Because of the iterative method that adaptive kernel estimation uses, running the function can take some time depending on the computer’s processor and the size of the sample.\n\n# Create density plot using adaptive kernel density estimation\nadapt = akj(x = vlss$age, z = d$x) \n\nTo plot this, we first dreate a data frame that includes the x-values used in the z= argument and the density estimates, stored in adapt$dens. Then we can use geom_line() to plot the actual density estimates.\n\n# Set up data frame\ndensity_est = data.frame(\n  x = d$x,\n  y = adapt$dens\n)\n\n# Plot density estimates\nggplot(data = density_est, aes(x = x, y = y)) +\n  geom_line() +\n  theme_bw() +\n  xlab(\"Age\") +\n  ylab(\"Density\")\n\n\n\n\nFigure 7: Density plot using adaptive kernel density estimation.",
    "crumbs": [
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "04-kde.html#density-estimation-in-practice",
    "href": "04-kde.html#density-estimation-in-practice",
    "title": "Kernel Density Estimation",
    "section": "Density Estimation in Practice",
    "text": "Density Estimation in Practice\nUnfortunately, there is no one uniform best method for choosing the smoothing parameter (Simonoff, 1996). Terrell (1990, p. 470) asserts that “most density estimates are presumably designed on aesthetic grounds: The practitioner picks the smoothing parameter so that the density looks good. Clearly, though, such an individualistic approach does not lead to replicable results; nor does it lead reliably to sensible estimates from a novice.”\nWhile researchers may make different analytic decisions about the choice of kernel and smoothing parameters, hopefully, the substantive findings about the research questions will not change (although it is possible that they may). For example, in all three methods of smoothing preseneted, the estimated kernel density shows that the population has three potential modes in the distribution. These likely refer to three distinct age groups or subpopulations. Each of these age groups has a higher density (taller distribution) and seems to have less variation (a thinner distribution) than the subsequent younger age group.\nRegardless of the analytic decisions made, any statistical results presented need to include sufficient information included to judge the soundness of the conclusions drawn. This helps the researcher either verify or call into question what she is seeing from the analysis. There are two things to keep in mind:\n\nThere is no “right” answer in statistical analysis (G. E. P. Box, 1979; George E. P. Box & Draper, 1987) but only explanation and presentation.\nThere is a distinction between how most researchers do their analysis and how they present their results.\n\nWhen presenting results, researchers should integrate or synthesize information from the data analysis with the content and findings from the substantive literature. Relating the findings back to previous research helps the researcher evaluate her findings. The results might verify other findings or question them. The researcher might also question her own results if they are not consistent with expectations and other studies. Remember, the statistical analysis is used to inform the substantive area, and the writing should reflect this. An example of a write-up for the age distribution analysis might be as follows.\n\nThe density for a sample of ages from \\(N=28,633\\) Vietnamese citizens was estimated using an adaptive kernel method (Portnoy & Koenker, 1989). The plot of the estimated kernel density reveals three potential subpopulations in the distribution which depict three distinct age groups: a younger, middle-aged, and older generation. The population pyramid based on the 1999 Vietnam census also seems to indicate the presence of three subpopulations (General Statistical Office, 2001). The overall positive skew in the distribution also suggests that the population of each subsequently higher age group has relatively lower frequency. Two other interesting features emerge in the density plot that have also been documented in the research literature (e.g., Haughton et al., 2001). The second mode in the age distribution occurs near the age of 40. This mode has lower frequency in part because of lives lost during the Vietnam War, which ended in 1975 with the reunification of North and South Vietnam. One can also see the effect of decreasing fertility rates in the fact that estimated density decreases as ages decrease from about 16 to zero.",
    "crumbs": [
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "04-kde.html#references",
    "href": "04-kde.html#references",
    "title": "Kernel Density Estimation",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBowman, A., & Azzalini, A. (1997). Applied smoothing techniques for data analysis: The kernel approach with S-Plus illustrations. Oxford University Press.\n\n\nBox, G. E. P. (1979). Robustness in the strategy of scientific model building. In R. L. Launer & G. N. Wilkinson (Eds.), Robustness in statistics (pp. 201–236). Academic Press.\n\n\nBox, George E. P., & Draper, N. R. (1987). Empirical model-building and response surfaces. Wiley.\n\n\nDeheuvels, P. (1977). Estimation nonparamètrique de la densitè par histogrammes gènèralisès. Rivista Di Statistica Applicata, 25, 5–42.\n\n\nEpanechnikov, V. A. (1969). Nonparametric estimation of a multivariate probability density. Theory of Probability and Its Applications, 14, 153–158.\n\n\nFreedman, D., & Diaconis, P. (1981). On the histogram as a density estimator: \\(\\mathrm{L}_2\\) theory. Zeitschrift Für Wahrscheinlichkeitstheorie Und Verwandte Gebiete, 57, 453–476.\n\n\nGeneral Statistical Office. (2001). Statistical yearbook. Statistical Publishing House.\n\n\nHaughton, D., Haughton, J., & Phong, N. (2001). Living standards during an economic boom: The case of vietnam. Statistical Publishing House; United Nations Development Programme.\n\n\nPortnoy, S., & Koenker, R. (1989). Adaptive L estimation of linear models. Annals of Statistics, 17, 362–381.\n\n\nRissanen, J., Speed, T., & Yu, B. (1992). Density estimation by stochastic complexity. IEEE Trans. On Information Theory, 38, 315–323.\n\n\nSain, S. R. (1994). Adaptive kernel density estimation [PhD thesis]. Rice University.\n\n\nScott, D. W. (1979). On optimal and data-based histograms. Biometrika, 66, 605–610.\n\n\nSheather, S. J. (2004). Density estimation. Statistical Science, 19(4), 588–597.\n\n\nSilverman, B. W. (1986). Density estimation for statistics and data analysis. Chapman; Hall.\n\n\nSimonoff, J. S. (1996). Smoothing methods in statistics. Springer.\n\n\nSturges, H. (1926). The choice of a class-interval. Journal of the American Statistical Association, 21, 65–66.\n\n\nTerrell, G. R. (1990). The maximal smoothing principle in density estimation. Journal of the American Statistical Association, 85(410), 470–477.\n\n\nTerrell, G. R., & Scott, D. W. (1992). Variable kernel density estimation. The Annals of Statistics, 20, 1236–1265.\n\n\nWand, M. P. (1997). Data-based choice of histogram bin width. The American Statistician, 51, 59–64.\n\n\nWorldBank. (1999). World development indicators 1999. CD-ROM; WorldBank.\n\n\nZieffler, A. S., Harring, J. R., & Long, J. D. (2011). Comparing groups: Randomization and bootstrap methods using R. Wiley.",
    "crumbs": [
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "04-kde.html#footnotes",
    "href": "04-kde.html#footnotes",
    "title": "Kernel Density Estimation",
    "section": "",
    "text": "The read_csv() function is technically included in the readr package which is included in the tidyverse package.↩︎\nThere are several kernel functions that can be used in density estimation.↩︎\nThe geom_density() function is technically included in the {ggplot2} package, which is included in the {tidyverse} package.↩︎",
    "crumbs": [
      "Kernel Density Estimation"
    ]
  },
  {
    "objectID": "05-exploration.html",
    "href": "05-exploration.html",
    "title": "Exploration: Comparing Two Groups",
    "section": "",
    "text": "Loading Packages and Importing the Data\nAdapted from Zieffler et al. (2011)\nIn the chapter on kernel density estimation the Vietnam Living Standards Survey (VLSS) was introduced. The survey was designed to provide an up-to-date source of data on households to be used in public policy formation, to assess current living standards, and to evaluate the impact of public programs. In this chapter, we will address the following research question:\nTo address these research questions, we will use the vlss-per-capita.csv to explore and compare the household expenditures across different demographic variables in the sample data.\nTo begin, we will load three packages that we will use in this analysis.\n# Load libraries\nlibrary(e1071)\nlibrary(tidyverse)\nThe data contains the household per capita expenditures for 5,999 households along with two demographic variables.\n# Import data\nvlss = read_csv(\"https://raw.githubusercontent.com/zief0002/musings/master/data/vlss-per-capita.csv\")\n# View data\nvlss\n\n# A tibble: 5,999 × 3\n   expend area  region\n    &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  184.  Rural      5\n 2   62.7 Rural      5\n 3  119.  Rural      5\n 4   76.6 Rural      5\n 5   97.5 Rural      5\n 6  132.  Rural      5\n 7   99.9 Rural      5\n 8   80.0 Rural      5\n 9   88.6 Rural      5\n10  161.  Rural      5\n# ℹ 5,989 more rows",
    "crumbs": [
      "Exploration: Comparing Two Groups"
    ]
  },
  {
    "objectID": "05-exploration.html#graphically-summarizing-the-marginal-distribution",
    "href": "05-exploration.html#graphically-summarizing-the-marginal-distribution",
    "title": "Exploration: Comparing Two Groups",
    "section": "Graphically Summarizing the Marginal Distribution",
    "text": "Graphically Summarizing the Marginal Distribution\nInitially, the distribution of the per capita expenditures for all 5,999 households—ignoring whether they are urban or rural—is examined. The distribution ignoring the population area is known as the marginal distribution. The syntax to create a plot of the kernel density estimate (KDE) for the marginal distribution of the per capita expenditures is show below.\n\n\n\n\n\nFigure 1: Kernel density estimate for the marginal distribution of household per capita expenditures.\n\n\n\n\n\n\n\n\nThe plot of the marginal distribution shown in Figure 1 indicates that the per capita expenditures data is right skewed, as the majority of household per capita expenditures pile up at the low end, and taper off moving to the right. This suggests that many of the households in Vietnam have a low per capita expenditure (around $100 U.S.). It also shows some households that may be potential outliers in the marginal distribution. These are households with very high expenditures relative to the rest of the households in the sample.",
    "crumbs": [
      "Exploration: Comparing Two Groups"
    ]
  },
  {
    "objectID": "05-exploration.html#graphically-summarizing-conditional-distributions",
    "href": "05-exploration.html#graphically-summarizing-conditional-distributions",
    "title": "Exploration: Comparing Two Groups",
    "section": "Graphically Summarizing Conditional Distributions",
    "text": "Graphically Summarizing Conditional Distributions\nExamining the marginal distribution is useful in an initial examination of the data, but it does not help in answering the research question about rural and urban differences. To help address the first research question, the distribution of per capita expenditures for each area must be examined separately. The distributions of per capita household expenditures for each area are called conditional distributions because they are defined conditional on area.\nTo graphically examine the conditional distributions, we will plot the KDE for the distribution of household per capita expenditures separately for the urban and rural households. There are many ways to do this using the {tidyverse} functionality. Below we will map the fill aesthetic to the area attribute in the dataframe. This will create the two KDEs in different colors in our plot. Because we are mapping an attribute in the data to an aesthetic, this is included inside the aes() function. The alpha=0.6 argument makes the density plots semi-transparent1 so that we can see both plots in the figure. The scale_fill_manual() layer changes the fill color used to shade in the density plots.\n\n\n\n\n\nFigure 2: Kernel density estimate for the distribution of household per capita expenditures conditioned on area.\n\n\n\n\n\n\n\n\nFigure 2 shows a single graph—or panel—with the conditional density curves superimposed and coded by line color. By having both conditional distributions in the same panel, this type of plot makes it psychologically easier for people to make comparisons. As can be seen in the plot, the urban curve is shifted to the right of the rural curve toward higher per capita household expenditures. In addition, the peak of the urban curve is lower than that of the rural curve, and both distributions are positively skewed.",
    "crumbs": [
      "Exploration: Comparing Two Groups"
    ]
  },
  {
    "objectID": "05-exploration.html#numerical-summaries-of-data-estimates-of-the-population-parameters",
    "href": "05-exploration.html#numerical-summaries-of-data-estimates-of-the-population-parameters",
    "title": "Exploration: Comparing Two Groups",
    "section": "Numerical Summaries of Data: Estimates of the Population Parameters",
    "text": "Numerical Summaries of Data: Estimates of the Population Parameters\nAfter graphically examining the data, it is desirable to obtain a more precise numerical summarization of the estimated population distribution. The numerical summaries can generally be split into two different types:\n\nMeasures of location, or central tendency\nMeasures of variability, or dispersion\n\nMeasures of location are single values that represent the measurement of a typical individual or unit in the distribution being studied. For example, in Figure 2, a typical household in the distribution might be defined as having a per capita expenditure at the dollar amount directly below the peak of the curve. Based on this, the typical urban household has a higher per capita expenditure than the typical rural household.\nMeasures of variability provide an indication of how different, or variable, the measurements in the distribution happen to be. For instance, Figure 2 also shows that the urban distribution spans a longer interval than the rural distribution, indicating urban household have more variation in their per capita expenditures than rural households. Researchers are often interested in the measures of location and variation in the population as they constitute relatively clear summaries of important aspects of distributions. The numerical summaries of the population distribution are called parameters. Parameters are estimated using sample data.\n\n\nMeasuring Central Tendency\nThe three most common measures of location are the mean, the median, and the mode. The mode describes a typical measurement in terms of the most common outcome or most frequently occurring score. In Figure 2, the mode of each distribution is the household expenditure value directly under the peak of the curve. A limitation in using the mode is that a distribution can have more than one. This indicates that the mode will not always have a unique value and, thus, cannot be recommended for general use.\nIn contrast to the mode, the median and mean are always unique values. The median is the middle-most score in a distribution. The median() function is used to find the median of the distribution. The best known and most frequently used measure of central tendency is the mean, or the average. The mean() function is used to find the mean of a distribution.\n\n\n## Marginal mean and median household per capita expenditure\nvlss |&gt;\n  summarize(\n    M = mean(expend),\n    Med = median(expend)\n    )\n\n# A tibble: 1 × 2\n      M   Med\n  &lt;dbl&gt; &lt;dbl&gt;\n1  213.  160.\n\n\nThe median household per capita expenditure is $160, and the mean household per capita expenditure is $213. In symmetric distributions, the mean and median can be equal or nearly so. However, in asymmetric distributions, the two can differ, sometimes drastically.\n\n\n\nConditional Means and Medians\nThe mean and median computed in the previous section summarize the marginal distribution, as area is ignored. Though the marginal estimates are useful, the goal is to compute the conditional estimates of a typical household per capita expenditure for each area. To do this we add a group_by() layer into our piping syntax prior to computing the summary values. Consider the following syntax:\n\n## Condition mean and median household per capita expenditure\nvlss |&gt;\n  group_by(area) |&gt;\n  summarize(\n    M = mean(expend),\n    Med = median(expend)\n    )\n\n# A tibble: 2 × 3\n  area      M   Med\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Rural  157.  135.\n2 Urban  349.  279.\n\n\nThe mean household per capita expenditure for the urban area is more than twice that for the rural area. This is consistent with Figure 2 that shows the urban distribution being right-shifted relative to the rural distribution. This suggests that the average household per capita expenditure differs for urban and rural areas in the sample.\n\n\n\nMeasuring Variation\nWhen an analysis deals with at least two groups, as in the rural/urban comparisons, it is important to consider group differences in variability and well as location. Variability within the groups influences the evaluation of location differences. High within-group variability can be an overwhelming feature that can render location differences as irrelevant, or at least less relevant. On the other hand, low within-group variability can work to accentuate location differences.\nConsider the examples in Figure 3. In both panels the mean difference between the distributions is the same. However, the large within-group variation in the rural distribution in the left-panel makes the interpretation of group differences less clear for these data than for the data shown in the right-panel. In fact, it can be argued that the most important feature is the fact that the urban distribution is almost entirely contained within the rural distributions. This means, for example, that though the rural mean is lower than the urban mean, there are several rural households that are higher than the urban mean, and some that are higher than any urban households.\nIn contrast, in the right-panel of Figure 3, there is essentially no overlap between the two distributions. This means that the mean difference also characterizes the difference between almost every pair of households from the two distributions. If we were to randomly select one rural and one urban household, the rural household would almost surely have a lower annual income. The same cannot be said of the overlapping distributions in the right-panel of Figure 3.\n\n\n\n\n\nFigure 3: Simulated density plots for the distribution of household per capita expenditures conditioned on area showing large within-group variation (LEFT PANEL) and small within-group variation (RIGHT PANEL).\n\n\n\n\n\n\n\n\nTwo summary measures of variation—the standard deviation and variance—are based on the deviations of the data from the mean. The sd() and var() functions can be used to compute these quantities, respectively. The syntax below illustrates the use of the functions to find the standard deviation for both the marginal and conditional distributions of household per capita expenditures.\n\n## SD and variance of the marginal distribution of household per capita expenditure\nvlss |&gt;\n  summarize(\n    SD = sd(expend),\n    Variance = var(expend)\n    )\n\n# A tibble: 1 × 2\n     SD Variance\n  &lt;dbl&gt;    &lt;dbl&gt;\n1  180.   32222.\n\n## SD and variance of the conditional distributions of household per capita expenditure\nvlss |&gt;\n  group_by(area) |&gt;\n  summarize(\n    SD = sd(expend),\n    Variance = var(expend)\n  )\n\n# A tibble: 2 × 3\n  area     SD Variance\n  &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 Rural  96.9    9385.\n2 Urban 250.    62564.\n\n\nBased on these conditional summaries, the rural households show less variation than the urban households. This is consistent with Figure 2 that shows the urban distribution being wider relative to the rural distribution. The average household per capita expenditure is more homogeneous for rural than for urban households. There are some caveats regarding indexes of variation. Most notably, measures of variation are sensitive to asymmetry, and their values can be inflated by even a single extreme value. For this reason, the skewness of the distributions should be considered when comparing measures of variation computed on such distributions.\nAnother measure of variation that often gets reported in the educational and behavioral sciences, is the standard error of the mean. The idea underlying the standard error is that different samples drawn from the same population have different values of the sample mean. This is a consequence of random sampling and the fact that sample information is always incomplete relative to the population. The standard error of the mean is the standard deviation of all the possible sample means for a given sample size. As such, this measure offers an indication of the precision of the sample mean, when it is used as an estimate of the population mean. The smaller the standard error the greater the precision. The standard error for the mean is computed as\n\\[\n\\mathrm{SE}_{\\bar{Y}}=\\frac{\\mathrm{SD}_Y}{\\sqrt{n}}.\n\\]\nwhere \\(\\mathrm{SD}_Y\\) is the standard deviation of the observed measurements on some variable \\(Y\\). The standard error of the mean is computed for both the urban and rural households in the syntax below. The standard error for the rural group is approximately four times smaller than that of the urban group (\\(\\frac{6.01}{1.48} \\approx 4\\)). This suggests that the sample mean for the rural households is a more precise estimate of the rural population mean than the sample urban mean is for the urban population. The use of the sample estimates and standard error for estimating population parameters is discussed further in Chapter 9.\n\nvlss |&gt;\n  group_by(area) |&gt;\n  summarize(\n    SD = sd(expend),\n    N = n()\n    ) |&gt;\n  mutate(\n    SE = SD / sqrt(N)\n  )\n\n# A tibble: 2 × 4\n  area     SD     N    SE\n  &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;\n1 Rural  96.9  4269  1.48\n2 Urban 250.   1730  6.01\n\n\n\n\n\nMeasuring Skewness\nSkewness is a numerical measure that helps summarize a distribution’s departure from symmetry about its mean. A completely symmetric distribution has a skewness value of zero.2 Positive values suggest a positively skewed (right-tailed) distribution with an asymmetric tail extending toward more positive values, whereas negative values suggest a negatively skewed (left-tailed) distribution with an asymmetric tail extending toward more negative values.\nThe {e1071} package3 provides a function called skewness(), which computes the skewness value for a sample distribution based on three common algorithms. This function is supplied with the argument type=2 to compute G1, a slightly modified version of skewness that is a better population estimate (e.g., Joanes & Gill, 1998). The syntax below shows the use of skewness() to find the G1 values for the conditional distributions.\n\n## Skewness (G1) of the conditional distributions of household per capita expenditure\nvlss |&gt;\n  group_by(area) |&gt;\n  summarize(\n    G1 = skewness(expend, type = 2) \n  )\n\n# A tibble: 2 × 2\n  area     G1\n  &lt;chr&gt; &lt;dbl&gt;\n1 Rural  4.28\n2 Urban  2.73\n\n\nThese values suggest that both the urban and rural distributions are positively skewed, but more so for the rural group. The following guidelines are offered as help in interpreting the skewness statistic. Like all guidelines these should be used with a healthy amount of skepticism. All statistics should be interpreted in terms of the types and purposes of the data analysis, as well as the substantive area of the research.\n\nIf \\(G_1=0\\), the distribution is symmetric.\nIf \\(\\left|G_1\\right| &lt; 1\\), the skewness of the distribution is slight.4\nIf \\(1 &lt; \\left|G_1\\right| &lt; 2\\), the the skewness of the distribution is moderate.\nIf \\(\\left|G_1\\right| &gt;2\\), the distribution is quite skewed.\n\nThe above guidelines indicate that both distributions in the example are severely positively skewed. Furthermore, the rural distribution is more asymmetric than the urban distribution. This is again consistent with Figure 2, which shows the rural distribution has a longer tail relative to its mean than the urban distribution. The distribution of rural households shows relatively less density for household per capita expenditures above the mean than below the mean. This asymmetry is even more evident for urban households.\n\n\n\nMeasuring Kurtosis\nKurtosis is often used as a numerical summarization of the “peakedness” of a distribution, referring to the relative concentration of scores in the center, tail, and shoulders. Normal distributions have a kurtosis value of zero and are called mesokurtic.5 Distributions that reflect a more peaked and heavy-tailed distribution than the normal distribution have positive kurtosis values, and are said to be leptokurtic. Distributions which are flatter and lighter-tailed than the normal distribution have negative kurtosis values and are said to be platykurtic. Dyson & Cantab (1943, p. 360) suggest an “amusing mnemonic”—which was attributed to Gossett (Student, 1927)—for the above terms:\n\nPlatykurtic curves, like the platypus, are squat with short tails. Leptokurtic curves are high with long tails, like the kangaroo—noted for “lepping”.\n\nThe left- and right-hand panels of Figure 4 depict distributions with different kurtosis values. The mesokurtic distribution is shown for a basis of comparison in both figures. The distributions in the left-hand panel show positive kurtosis, whereas the distributions in the right-hand panel show negative kurtosis.\n\n\n\n\n\nFigure 4: LEFT PANEL: Kernel density estimate for a mesokurtic distribution (dashed, orange line) and a leptokurtic distribution (solid, purple line). The leptokurtic distributions are skinnier and more peaked than the mesokurtic distribution. RIGHT PANEL: Kernel density estimate for a mesokurtic distribution (dashed, orange line) and a platykurtic distribution (solid, blue line). The platykurtic distribution is flatter than the mesokurtic distribution.\n\n\n\n\n\n\n\n\nThe kurtosis() function provided in the {e1071} package can be used to compute the sample kurtosis value for a distribution based on three common algorithms. We use this function with the argument type=2 to compute G2, a slightly modified version of the kurtosis statistic that is a better population estimate of kurtosis (e.g., Joanes & Gill, 1998). The syntax below shows the use of kurtosis() to find the G2 values for the conditional distributions.\n\n## Kurtosis (G2) of the conditional distributions of household per capita expenditure\nvlss |&gt;\n  group_by(area) |&gt;\n  summarize(\n    G2 = kurtosis(expend, type = 2) \n  )\n\n# A tibble: 2 × 2\n  area     G2\n  &lt;chr&gt; &lt;dbl&gt;\n1 Rural  42.7\n2 Urban  14.0\n\n\nThe kurtosis statistics for the conditional distributions suggest that both distributions are severely leptokurtic indicating that these distributions are more peaked than a normal distribution. They also have more density in the tails of the distribution than we would expect to see in a normal distribution. One can see in Figure 2 that the rural distribution is even more peaked than the urban distribution.\nWhile the kurtosis statistic is often examined and reported by educational and behavioral scientists who want to numerically describe their samples, it should be noted that “there seems to be no universal agreement about the meaning and interpretation of kurtosis” (Moors, 1986, p. 283). Most textbooks in the social sciences describe kurtosis in terms of peakedness and tail weight. Balanda & MacGillivray (1988, p. 116) define kurtosis as “the location- and scale free movement of probability mass from the shoulders of a distribution into its center and tails”. Other statisticians have suggested that it is a measure of the bimodality present in a distribution (e.g., Darlington, 1970; Finucan, 1964). Perhaps it is best defined by Mosteller & Tukey (1977), who suggest that like location, variation, and skewness, kurtosis should be viewed as a “vague concept” that can be formalized in a variety of ways.",
    "crumbs": [
      "Exploration: Comparing Two Groups"
    ]
  },
  {
    "objectID": "05-exploration.html#summarizing-the-findings",
    "href": "05-exploration.html#summarizing-the-findings",
    "title": "Exploration: Comparing Two Groups",
    "section": "Summarizing the Findings",
    "text": "Summarizing the Findings\nThe APA manual (American Psychological Association, 2019) provides suggestions for presenting descriptive statistics for groups of individuals. The information should be presented in the text when there are three or fewer groups and in a table when there are more than three groups. While this number is not set in stone, we want to present results in a manner that will facilitate understanding. Typically we report measures of location, variation, and sample size for each group, at the very least. We present the results of our data analysis below.\n\nEmpirical evidence on the process of urbanization has shown increased economic segregation among urban and rural households, as well as increased spatial differentiation of land uses (e.g., Leaf, 2002). The Socialist Republic of Vietnam, for the last decade, has experienced an industrialization characterized by economic growth and urbanization.\nStatistical analysis shows that the typical household per capita expenditure is higher for urban households (\\(M=\\$349\\), \\(\\mathrm{SE}=\\$6\\)) than for rural households (\\(M=\\$157\\), \\(\\mathrm{SE}=\\$1\\)). The distribution for urban households (\\(\\mathrm{SD}=\\$250\\)) also shows more variation than the distribution for rural households (\\(\\mathrm{SD}=\\$97\\)) indicating that rural areas tend to be more homogeneous in their household per capita expenditures. This evidence is further strengthened by the difference in asymmetry and heavy-tailedness in the urban (\\(G1=2.73\\), \\(G2=42.66\\)) and rural (\\(G1=4.28\\), \\(G2=14.03\\)) distributions.\nIn contrast to their urban counterparts, the economic stimulation in rural areas of Vietnam seems not to have been as dynamic. The typical household for rural areas is only $15 U.S. above the poverty line. Furthermore, except for a rather small number of wealthier rural households, the majority of rural households show little variation in their household per capita expenditures. This shared level of poverty could be due to the fact that a substantial share of the populace living in rural areas of Vietnam are now unemployed or underemployed.\nIt is worth noting, that the poverty line—established in 1998 by the General Statistical Office at $119 U.S. (General Statistical Office, 2001)—is close to the mode of the rural expenditure per capita distribution, which could indicate that a small increase in household expenditure per capita is enough to shift many of the rural households to a position above the poverty line. This is one likely explanation for recent dramatic reductions in poverty rates in Vietnam. As the poverty line moves higher, further reductions in poverty rates are likely to be smaller in magnitude.",
    "crumbs": [
      "Exploration: Comparing Two Groups"
    ]
  },
  {
    "objectID": "05-exploration.html#references",
    "href": "05-exploration.html#references",
    "title": "Exploration: Comparing Two Groups",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nAmerican Psychological Association. (2019). Publication manual of the American Psychological Association (7th ed.). Author.\n\n\nBalanda, K. P., & MacGillivray, H. L. (1988). Kurtosis: A critical review. The American Statistician, 42(2), 111–119.\n\n\nDarlington, R. B. (1970). Is kurtosis really “peakedness?” The American Statistician, 24(2), 19–22.\n\n\nDyson, F. J., & Cantab, B. A. (1943). A note on kurtosis. Journal of the Royal Statistical Society, 106(4), 360–361.\n\n\nFinucan, H. M. (1964). A note on kurtosis. Journal of the Royal Statistical Society. Series B (Methodological), 26(1), 111–112.\n\n\nGeneral Statistical Office. (2001). Statistical yearbook. Statistical Publishing House.\n\n\nJoanes, D. N., & Gill, C. A. (1998). Comparing measures of sample skewness and kurtosis. The Statistician, 47(Part 1), 183–189.\n\n\nLeaf, M. (2002). A tale of two villages: Globalization and peri-urban change in china and vietnam. Cities, 19(1), 23–31.\n\n\nMoors, J. J. A. (1986). The meaning of kurtosis: Darlington reexamined. The American Statistician, 40(4), 283–284.\n\n\nMosteller, F., & Tukey, J. W. (1977). Data analysis and regression. Addison-Wesley.\n\n\nStudent. (1927). Errors of routine analysis. Biometrika, 19(1/2), 151–164.\n\n\nZieffler, A. S., Harring, J. R., & Long, J. D. (2011). Comparing groups: Randomization and bootstrap methods using R. Wiley.",
    "crumbs": [
      "Exploration: Comparing Two Groups"
    ]
  },
  {
    "objectID": "05-exploration.html#footnotes",
    "href": "05-exploration.html#footnotes",
    "title": "Exploration: Comparing Two Groups",
    "section": "",
    "text": "Any value from 0–1 can be used, where alpha=0 is fully transparent and alpha=1 is fully opaque.↩︎\nTechnically this is only true for an index of skewness that has been “corrected” or “standardized” so that the normal distribution has a skewness of zero. Skewness indices need not be zero for a normal distribution in general.↩︎\nAn alternative package is {moments}.↩︎\n\\(\\left|G_1\\right|\\) indicates the absolute value of \\(G1\\) (cut off the sign).↩︎\nAgain, technically this is only true for indices of kurtosis that have been “corrected” so that a normal distribution has a kurtosis of zero.↩︎",
    "crumbs": [
      "Exploration: Comparing Two Groups"
    ]
  }
]